{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = torchvision.datasets.MNIST(root=\"./data\",train=True,download=True,transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ]))\n",
    "mnist_testset = torchvision.datasets.MNIST(root=\"./data\",train=False,download=True,transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "$28\\times28$ images with labels 0-9\n",
    "\n",
    "image $i$: mnist_trainset[$i$][0][0]\n",
    "\n",
    "label $i$: mnist_trainset[$i$][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.e-5\n",
    "\n",
    "def log_categorical(x, p, num_classes=10):\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    return log_p\n",
    "\n",
    "def log_normal_diag(x, mu, log_var):\n",
    "    D = x.shape[0]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    return log_p\n",
    "\n",
    "def log_standard_normal(x):\n",
    "    D = x.shape[0]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * x**2.\n",
    "    return log_p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    def reparameterization(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu+std*eps\n",
    "    \n",
    "    def encode(self,x):\n",
    "        h_e = self.encoder(x.view(x.shape[0],28**2))\n",
    "        mu_e, log_var_e = torch.chunk(h_e,chunks=2,dim=1)\n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    def sample(self, mu_e, log_var_e):\n",
    "        z = self.reparameterization(mu_e,log_var_e)\n",
    "        return z\n",
    "    \n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        else:\n",
    "            if (mu_e is None) or (log_var_e is None) or (z is None):\n",
    "                raise ValueError('mu, log-var and z can`t be None!')\n",
    "\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.log_prob(x) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,decoder_net,num_vals):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.num_vals = num_vals\n",
    "        self.decoder = decoder_net\n",
    "    \n",
    "    def decode(self,z):\n",
    "        h_d = self.decoder(z)\n",
    "        h_d = h_d.view(z.shape[0],28,28,self.num_vals)\n",
    "        mu_d = torch.softmax(h_d,dim=2)\n",
    "        return mu_d\n",
    "\n",
    "    def sample(self,z):\n",
    "        mu_d = self.decode(z)\n",
    "        mu_d = mu_d.view(28, -1, self.num_vals)\n",
    "        p = mu_d.view(-1, self.num_vals)\n",
    "        x_new = torch.multinomial(p, num_samples = 1).view(28, 28)\n",
    "        return x_new\n",
    "        \n",
    "    def log_prob(self, x, z):\n",
    "        mu_d = self.decode(z)\n",
    "        log_p = torch.sum(log_categorical(x, mu_d, num_classes=self.num_vals),dim=-1)\n",
    "        return log_p\n",
    "    \n",
    "    def forward(self, z, x=None):\n",
    "        return self.log_prob(x, z)        \n",
    "        \n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "    def __init__(self,L):\n",
    "        super(Prior, self).__init__()\n",
    "        self.L = L\n",
    "    \n",
    "    def sample(self, batchsize=1):\n",
    "        z = torch.randn((batchsize,self.L))\n",
    "        return z\n",
    "    \n",
    "    def log_prob(self, z):\n",
    "        return log_standard_normal(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,encoder_net,decoder_net,L,num_vals):\n",
    "        super(VAE,self).__init__()\n",
    "        self.encoder = Encoder(encoder_net)\n",
    "        self.decoder = Decoder(decoder_net,num_vals)\n",
    "        self.Prior = Prior(L)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        z = self.encoder.sample(mu_e,log_var_e)\n",
    "        RE = self.decoder.log_prob(x,z)\n",
    "        KL = self.Prior.log_prob(z)\n",
    "        KL -= self.encoder.log_prob(mu_e=mu_e,log_var_e=log_var_e,z=z)\n",
    "        ELBO = -(RE+KL.sum(-1)).sum()\n",
    "        return ELBO.mean()\n",
    "    \n",
    "    def sample(self, batchsize = 1):\n",
    "        z = self.Prior.sample(batchsize)\n",
    "        return self.decoder.sample(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, model):\n",
    "    loss = 0; N = 0\n",
    "    for batch, num in test_loader:\n",
    "            loss_it = model.forward(batch)\n",
    "            loss += loss_it\n",
    "            N += batch.shape[0]\n",
    "    return loss / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    for e in range(num_epochs):\n",
    "        model.train()\n",
    "        for indx_batch, (batch,target) in enumerate(training_loader):\n",
    "            loss = model.forward(batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph = True)\n",
    "            optimizer.step()\n",
    "            if indx_batch % 1000 == 0:\n",
    "                print(indx_batch)\n",
    "                loss_val = evaluation(val_loader,model)\n",
    "                print(\"Epoch: \" + str(e) + \" Training loss: \" + str(loss) + \" Validation loss: \"+ str(loss_val))\n",
    "                nll_val.append(loss_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "train_set, val_set, _ = random_split(mnist_trainset,[9000,1000,50000])\n",
    "training_loader = DataLoader(train_set,batch_size = 1,shuffle=True)\n",
    "validation_loader = DataLoader(val_set,batch_size = 1,shuffle=True)\n",
    "test_loader = DataLoader(mnist_testset,batch_size = 1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 0loss: tensor(2754.2993, grad_fn=<MeanBackward0>)val loss: tensor(3032.0784, grad_fn=<DivBackward0>)\n",
      "1000\n",
      "Epoch: 0loss: tensor(2197.4233, grad_fn=<MeanBackward0>)val loss: tensor(2602.1475, grad_fn=<DivBackward0>)\n",
      "2000\n",
      "Epoch: 0loss: tensor(2596.1167, grad_fn=<MeanBackward0>)val loss: tensor(2609.1353, grad_fn=<DivBackward0>)\n",
      "3000\n",
      "Epoch: 0loss: tensor(2412.6475, grad_fn=<MeanBackward0>)val loss: tensor(2560.6807, grad_fn=<DivBackward0>)\n",
      "4000\n",
      "Epoch: 0loss: tensor(2486.8999, grad_fn=<MeanBackward0>)val loss: tensor(2564.7078, grad_fn=<DivBackward0>)\n",
      "5000\n",
      "Epoch: 0loss: tensor(2485.1074, grad_fn=<MeanBackward0>)val loss: tensor(2545.8787, grad_fn=<DivBackward0>)\n",
      "6000\n",
      "Epoch: 0loss: tensor(2733.9297, grad_fn=<MeanBackward0>)val loss: tensor(2550.6716, grad_fn=<DivBackward0>)\n",
      "7000\n",
      "Epoch: 0loss: tensor(2369.0459, grad_fn=<MeanBackward0>)val loss: tensor(2552.0242, grad_fn=<DivBackward0>)\n",
      "8000\n",
      "Epoch: 0loss: tensor(2624.0723, grad_fn=<MeanBackward0>)val loss: tensor(2550.5723, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "D = 28**2\n",
    "hidden_layer = 128\n",
    "L = 10\n",
    "num_values = 10\n",
    "encoder = nn.Sequential(nn.Linear(D,hidden_layer),nn.ReLU(),nn.Linear(hidden_layer,2*L))\n",
    "decoder = nn.Sequential(nn.Linear(L,hidden_layer),nn.ReLU(),nn.Linear(hidden_layer,D*L))\n",
    "model = VAE(encoder, decoder, L, num_values)\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)\n",
    "nll_val = training(num_epochs=1,model=model,training_loader=training_loader,val_loader=validation_loader,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

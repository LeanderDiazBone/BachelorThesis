{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import scvi\n",
    "from scib_metrics.benchmark import Benchmarker\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e68ae4df566424487f6fa836aed0f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/972M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adata = sc.read(\n",
    "    \"data/lung_atlas.h5ad\",\n",
    "    backup_url=\"https://figshare.com/ndownloader/files/24539942\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor=\"cell_ranger\", batch_key=\"batch\")\n",
    "sc.tl.pca(adata, n_comps=30, use_highly_variable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[:, adata.var.highly_variable].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm[\"Unintegrated\"] = adata.obsm[\"X_pca\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scanorama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscanorama\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# List of adata per batch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m batch_cats \u001b[39m=\u001b[39m adata\u001b[39m.\u001b[39mobs\u001b[39m.\u001b[39mbatch\u001b[39m.\u001b[39mcat\u001b[39m.\u001b[39mcategories\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scanorama'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import scanorama\n",
    "\n",
    "# List of adata per batch\n",
    "batch_cats = adata.obs.batch.cat.categories\n",
    "adata_list = [adata[adata.obs.batch == b].copy() for b in batch_cats]\n",
    "scanorama.integrate_scanpy(adata_list)\n",
    "\n",
    "adata.obsm[\"Scanorama\"] = np.zeros((adata.shape[0], adata_list[0].obsm[\"X_scanorama\"].shape[1]))\n",
    "for i, b in enumerate(batch_cats):\n",
    "    adata.obsm[\"Scanorama\"][adata.obs.batch == b] = adata_list[i].obsm[\"X_scanorama\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/numba/np/ufunc/dufunc.py:84: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/numba/np/ufunc/dufunc.py:84: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/numba/np/ufunc/dufunc.py:84: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/umap/plot.py:203: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/envs/scvi-tools-dev-n/lib/python3.11/site-packages/pyliger/preprocessing/_scale.py:88: RuntimeWarning: divide by zero encountered in divide\n",
      "100%|██████████| 30/30 [06:30<00:00, 13.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import pyliger\n",
    "\n",
    "bdata = adata.copy()\n",
    "# Pyliger normalizes by library size with a size factor of 1\n",
    "# So here we give it the count data\n",
    "bdata.X = bdata.layers[\"counts\"]\n",
    "# List of adata per batch\n",
    "adata_list = [bdata[bdata.obs.batch == b].copy() for b in batch_cats]\n",
    "for i, ad in enumerate(adata_list):\n",
    "    ad.uns[\"sample_name\"] = batch_cats[i]\n",
    "    # Hack to make sure each method uses the same genes\n",
    "    ad.uns[\"var_gene_idx\"] = np.arange(bdata.n_vars)\n",
    "\n",
    "\n",
    "liger_data = pyliger.create_liger(adata_list, remove_missing=False, make_sparse=False)\n",
    "# Hack to make sure each method uses the same genes\n",
    "liger_data.var_genes = bdata.var_names\n",
    "pyliger.normalize(liger_data)\n",
    "pyliger.scale_not_center(liger_data)\n",
    "pyliger.optimize_ALS(liger_data, k=30)\n",
    "pyliger.quantile_norm(liger_data)\n",
    "\n",
    "\n",
    "adata.obsm[\"LIGER\"] = np.zeros((adata.shape[0], liger_data.adata_list[0].obsm[\"H_norm\"].shape[1]))\n",
    "for i, b in enumerate(batch_cats):\n",
    "    adata.obsm[\"LIGER\"][adata.obs.batch == b] = liger_data.adata_list[i].obsm[\"H_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad(filename=\"data/adataw.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = anndata.read_h5ad(filename=\"data/adataw.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(adata, prior, prior_kwargs = None, max_epochs = 100):\n",
    "    scvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=\"batch\")\n",
    "    vae = scvi.model.SCVI(adata, prior_distribution = prior,prior_kwargs=prior_kwargs, n_layers=2, n_latent=30)\n",
    "    vae.train(max_epochs=max_epochs)\n",
    "    adata.obsm[\"scVI\"] = vae.get_latent_representation()\n",
    "    return adata\n",
    "\n",
    "def plotBenchmarkResults(adata):\n",
    "    bm = Benchmarker(\n",
    "    adata,\n",
    "    batch_key=\"batch\",\n",
    "    label_key=\"cell_type\",\n",
    "    embedding_obsm_keys=[\"Unintegrated\", \"Scanorama\", \"LIGER\", \"Harmony\", \"scVI\", \"scANVI\"],\n",
    "    n_jobs=6,\n",
    "    )\n",
    "    bm.benchmark()\n",
    "    bm.plot_results_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/data/scvi-tools/scvi/data/fields/_base_field.py:64: UserWarning: adata.layers[counts] does not contain unnormalized count data. Are you sure this is what you want?\n",
      "  self.validate_field(adata)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m adataSD \u001b[39m=\u001b[39m anndata\u001b[39m.\u001b[39mread_h5ad(filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/adataw.h5ad\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m sdnormalAdata \u001b[39m=\u001b[39m trainModel(adataSD, \u001b[39m\"\u001b[39;49m\u001b[39msdnormal\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(adata, prior, prior_kwargs, max_epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m scvi\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mSCVI\u001b[39m.\u001b[39msetup_anndata(adata, layer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcounts\u001b[39m\u001b[39m\"\u001b[39m, batch_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m vae \u001b[39m=\u001b[39m scvi\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mSCVI(adata, prior_distribution \u001b[39m=\u001b[39m prior,prior_kwargs\u001b[39m=\u001b[39mprior_kwargs, n_layers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, n_latent\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m vae\u001b[39m.\u001b[39;49mtrain(max_epochs\u001b[39m=\u001b[39;49mmax_epochs)\n\u001b[1;32m      5\u001b[0m adata\u001b[39m.\u001b[39mobsm[\u001b[39m\"\u001b[39m\u001b[39mscVI\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39mget_latent_representation()\n\u001b[1;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m adata\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/model/base/_training_mixin.py:88\u001b[0m, in \u001b[0;36mUnsupervisedTrainingMixin.train\u001b[0;34m(self, max_epochs, use_gpu, accelerator, devices, train_size, validation_size, shuffle_set_split, batch_size, early_stopping, plan_kwargs, **trainer_kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m trainer_kwargs[es] \u001b[39m=\u001b[39m (\n\u001b[1;32m     76\u001b[0m     early_stopping \u001b[39mif\u001b[39;00m es \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m trainer_kwargs\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m trainer_kwargs[es]\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     78\u001b[0m runner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_runner_cls(\n\u001b[1;32m     79\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     80\u001b[0m     training_plan\u001b[39m=\u001b[39mtraining_plan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrainer_kwargs,\n\u001b[1;32m     87\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m runner()\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/train/_trainrunner.py:99\u001b[0m, in \u001b[0;36mTrainRunner.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_splitter, \u001b[39m\"\u001b[39m\u001b[39mn_val\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_plan\u001b[39m.\u001b[39mn_obs_validation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_splitter\u001b[39m.\u001b[39mn_val\n\u001b[0;32m---> 99\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_plan, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_splitter)\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history()\n\u001b[1;32m    102\u001b[0m \u001b[39m# data splitter only gets these attrs after fit\u001b[39;00m\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/train/_trainer.py:186\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], PyroTrainingPlan):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\n\u001b[1;32m    182\u001b[0m         action\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    183\u001b[0m         category\u001b[39m=\u001b[39m\u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m    184\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`LightningModule.configure_optimizers` returned `None`\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 186\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:531\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    529\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 531\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    533\u001b[0m )\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:570\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    561\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    562\u001b[0m )\n\u001b[1;32m    564\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    565\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    566\u001b[0m     ckpt_path,\n\u001b[1;32m    567\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    568\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m )\n\u001b[0;32m--> 570\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    572\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    573\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:975\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    972\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1017\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1018\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1019\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    353\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    217\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], kwargs)\n\u001b[1;32m    219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         closure()\n\u001b[1;32m    180\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    187\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:260\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    259\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    261\u001b[0m     trainer,\n\u001b[1;32m    262\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    263\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    264\u001b[0m     batch_idx,\n\u001b[1;32m    265\u001b[0m     optimizer,\n\u001b[1;32m    266\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:140\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 140\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    142\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    143\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1256\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1219\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1220\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1224\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1225\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[39m    calls the optimizer.\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[39m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:155\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:225\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 225\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision_plugin.py:114\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/torch/optim/adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 121\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    124\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision_plugin.py:101\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     90\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     91\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     93\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     94\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     95\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:307\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    306\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:287\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    290\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:367\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m    366\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/train/_trainingplans.py:342\u001b[0m, in \u001b[0;36mTrainingPlan.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mkl_weight\u001b[39m\u001b[39m\"\u001b[39m: kl_weight})\n\u001b[1;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mkl_weight\u001b[39m\u001b[39m\"\u001b[39m, kl_weight, on_step\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, on_epoch\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 342\u001b[0m _, _, scvi_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(batch, loss_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_kwargs)\n\u001b[1;32m    343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m, scvi_loss\u001b[39m.\u001b[39mloss, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_and_log_metrics(scvi_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_metrics, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/train/_trainingplans.py:278\u001b[0m, in \u001b[0;36mTrainingPlan.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    277\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Passthrough to the module's forward method.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/module/base/_decorators.py:32\u001b[0m, in \u001b[0;36mauto_move_data.<locals>.auto_transfer_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# decorator only necessary after training\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m device \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m({p\u001b[39m.\u001b[39mdevice \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters()})\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(device) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/module/base/_base_module.py:199\u001b[0m, in \u001b[0;36mBaseModuleClass.forward\u001b[0;34m(self, tensors, get_inference_input_kwargs, get_generative_input_kwargs, inference_kwargs, generative_kwargs, loss_kwargs, compute_loss)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m@auto_move_data\u001b[39m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    167\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39m|\u001b[39m \u001b[39mtuple\u001b[39m[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, LossOutput]\n\u001b[1;32m    178\u001b[0m ):\n\u001b[1;32m    179\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward pass through the network.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39m        another return value.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m _generic_forward(\n\u001b[1;32m    200\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    201\u001b[0m         tensors,\n\u001b[1;32m    202\u001b[0m         inference_kwargs,\n\u001b[1;32m    203\u001b[0m         generative_kwargs,\n\u001b[1;32m    204\u001b[0m         loss_kwargs,\n\u001b[1;32m    205\u001b[0m         get_inference_input_kwargs,\n\u001b[1;32m    206\u001b[0m         get_generative_input_kwargs,\n\u001b[1;32m    207\u001b[0m         compute_loss,\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/module/base/_base_module.py:749\u001b[0m, in \u001b[0;36m_generic_forward\u001b[0;34m(module, tensors, inference_kwargs, generative_kwargs, loss_kwargs, get_inference_input_kwargs, get_generative_input_kwargs, compute_loss)\u001b[0m\n\u001b[1;32m    747\u001b[0m generative_outputs \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39mgenerative(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerative_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerative_kwargs)\n\u001b[1;32m    748\u001b[0m \u001b[39mif\u001b[39;00m compute_loss:\n\u001b[0;32m--> 749\u001b[0m     losses \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49mloss(\n\u001b[1;32m    750\u001b[0m         tensors, inference_outputs, generative_outputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mloss_kwargs\n\u001b[1;32m    751\u001b[0m     )\n\u001b[1;32m    752\u001b[0m     \u001b[39mreturn\u001b[39;00m inference_outputs, generative_outputs, losses\n\u001b[1;32m    753\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/local/data/scvi-tools/scvi/module/_vae.py:500\u001b[0m, in \u001b[0;36mVAE.loss\u001b[0;34m(self, tensors, inference_outputs, generative_outputs, kl_weight)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39m# qz: variational posterior, pz: prior\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior_distribution \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39msdnormal\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mnormal\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 500\u001b[0m     kl_divergence_z \u001b[39m=\u001b[39m kl(inference_outputs[\u001b[39m\"\u001b[39;49m\u001b[39mqz\u001b[39;49m\u001b[39m\"\u001b[39;49m], generative_outputs[\u001b[39m\"\u001b[39;49m\u001b[39mpz\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     log_q_zx \u001b[39m=\u001b[39m inference_outputs[\u001b[39m\"\u001b[39m\u001b[39mqz\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlog_prob(inference_outputs[\u001b[39m\"\u001b[39m\u001b[39mz\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/torch/distributions/kl.py:172\u001b[0m, in \u001b[0;36mkl_divergence\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m fun \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo KL(p || q) is implemented for p type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and q type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m                               \u001b[39m.\u001b[39mformat(p\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, q\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m fun(p, q)\n",
      "File \u001b[0;32m/local/data/anaconda3/envs/BT/lib/python3.11/site-packages/torch/distributions/kl.py:409\u001b[0m, in \u001b[0;36m_kl_normal_normal\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39m@register_kl\u001b[39m(Normal, Normal)\n\u001b[1;32m    408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_kl_normal_normal\u001b[39m(p, q):\n\u001b[0;32m--> 409\u001b[0m     var_ratio \u001b[39m=\u001b[39m (p\u001b[39m.\u001b[39;49mscale \u001b[39m/\u001b[39;49m q\u001b[39m.\u001b[39;49mscale)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\n\u001b[1;32m    410\u001b[0m     t1 \u001b[39m=\u001b[39m ((p\u001b[39m.\u001b[39mloc \u001b[39m-\u001b[39m q\u001b[39m.\u001b[39mloc) \u001b[39m/\u001b[39m q\u001b[39m.\u001b[39mscale)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\n\u001b[1;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (var_ratio \u001b[39m+\u001b[39m t1 \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m var_ratio\u001b[39m.\u001b[39mlog())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "adataSD = anndata.read_h5ad(filename=\"data/adataw.h5ad\")\n",
    "\n",
    "sdnormalAdata = trainModel(adataSD, \"sdnormal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "name 'plotBenchmarkResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plotBenchmarkResults(sdnormalAdatadata)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plotBenchmarkResults' is not defined"
     ]
    }
   ],
   "source": [
    "plotBenchmarkResults(sdnormalAdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalAdata = trainModel(anndata.read_h5ad(filename=\"data/adataw.h5ad\"), \"normal\")\n",
    "vampAdata = trainModel(anndata.read_h5ad(filename=\"data/adataw.h5ad\"), \"vamp\")\n",
    "mogAdata = trainModel(anndata.read_h5ad(filename=\"data/adataw.h5ad\"), \"mixofgaus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi-tools-dev-n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
